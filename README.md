# Batch vs Stochastic Gradient Descent

This project demonstrates the difference between **Batch Gradient Descent (BGD)** and **Stochastic Gradient Descent (SGD)** by training a machine learning model and analyzing how each optimization technique updates weights and minimizes loss.

---

## Objective
To understand how different gradient descent strategies affect:
- Model convergence
- Training stability
- Speed of learning

---

## What This Notebook Covers
- Conceptual explanation of Batch vs Stochastic Gradient Descent
- Training a model using Batch Gradient Descent
- Training the same model using Stochastic Gradient Descent
- Comparison of:
  - Loss reduction
  - Convergence behavior
  - Weight updates

---

## Key Learnings
- Batch Gradient Descent uses the entire dataset to update weights, leading to stable but slower convergence.
- Stochastic Gradient Descent updates weights using a single data point, resulting in faster but noisier convergence.
- Trade-offs between stability and speed in optimization techniques.

---

## Tech Stack
- Python
- NumPy
- Matplotlib
- Scikit-learn

---

## Project Purpose
This project strengthens the understanding of **optimization techniques** used in machine learning and deep learning models, which is essential for training neural networks efficiently.


